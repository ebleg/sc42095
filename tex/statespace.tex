In this section, the controller design will be approached from a state-space perspective. First, a full-information state feedback controller will be designed for a servo problem in \cref{sec:ss_full}. Then, the assumption of full-information will be dropped; an observer will also be designed using pole-placement to provide the state information; this is discussed in \cref{sec:ss_output}. Finally, a linear quadratic regulator (LQR) solution is developed in \cref{sec:ss_lqr}.

\subsection{Full-information feedback using pole-placement\textnormal{\phantom{xxx}(Question 5)}}
\label{sec:ss_full}
In order to apply full-information feedback, information about all the states must be available. Using a feedback law $u = -Lx$ where $L$ is a vector to be designed to obtain the desired closed-loop behaviour. \Cref{fig:q5_block_statefeedback} visualises the process of state feedback in a block diagram for a servo problem. Please note that a feedforward gain $L_c$ is included as well which acts on the reference signal $r$; its purpose is to adjust the DC gain of the system to remove the steady-state error --- this matter will be discussed as well.
\begin{figure}[ht]
    \centering
    \includegraphics[scale=1.1]{media/q5/block_statefeedback-01.eps}
    \caption{}
    \label{fig:q5_block_statefeedback}
\end{figure}
The closed-loop state-space system in \cref{fig:q5_block_statefeedback}  (the feedthrough $D$ is assumed to be absent):
\begin{equation}
    \begin{aligned}
        x(k+1) &= \qty(\Phi - \Gamma L)x(k) + \Gamma L_c r(k)\\
        y(k) &= Cx(k)\\
    \end{aligned}
\end{equation}
According to the pole-placement theorem, the all poles of the system can be moved to any location of choice, provided that the system is reachable. Because the $\Phi$ matrix is invertible (this is always the case for sampled continuous-time systems, which cannot have a pole at $s = -\infty$), controllability and reachability are equivalent statements for this system. The reachability condition can be checked using the rank of the controllability matrix:
$$ W_c = \mqty(\Gamma & \Phi \Gamma & \cdots & \Phi^{n-1}\Gamma)$$
The system is reachable if and only if $\mathrm{rank}(W_c) = n$ ($n$ is the number of states), which is indeed the case for this system. As such, one can move all the poles to arbitrary locations using state-feedback.

\paragraph{Selection of the pole locations} Obviously, the selection of the closed-loop poles will be of major influence on the behaviour of the system. Therefore, it is important to choose them wisely. For the controller design, these locations where picked in the $s$-plane for continous-time systems, after which they are mapped to the $z$-plane for discrete-time systems using the relation:
$$ p_z = \exp(hp_s)$$
where $h$ is the sampling period, $p_z$ the pole location in the $z$-plane and $p_s$ the pole location in the $s$-plane. In the $s$-plane, the characteristics of the poles are very straightforward to compute:
$$ \omega = \abs{p_s} \qquad \zeta = -\frac{\Re{p_s}}{\abs{p_s}}$$
The general idea is to choose a dominant pole pair that will induce a second-order response with the desired $\zeta$ and $\omega$, and place the other pole(s) at a faster location; the result will then be approximately equal to the `design' response because the other modes will die out much quicker.
\sisetup{scientific-notation=false}
Using \cref{eq:overshoot,eq:damping}, the desired damping ratio can be computed based on the overshoot requirement. From the tuning of the PID controllers in \cref{sec:continuoustracking,sec:continuousdisturbance} it is already known that the maximum imposed overshoot limit is not desired for minimum settling time; rather, the overshoot should be limited to 1\% to keep the system immediately inside the `settling band' of $\pm 1\%$. Using these relations, one can find that the desired $\zeta = 0.83$. To have minimal settling time, $\omega$ should be as large as possible. However, this will also directly influence the distance over which the poles have to be moved and therefore the required control action. The settling time can, for second-order systems, be related to $\zeta$ and $\omega$ with the following expression (for continuous-time systems): \cite{nise}
$$ T_s = -\frac{\ln(0.01\times\sqrt{1 - \zeta^2})}{\zeta\omega}$$
For the computed $\zeta$, an $\omega$ of \SI{25}{\radian\per\second} would yield a settling time of \SI{0.25}{\second}, comparable to the PD controller from \cref{sec:continuoustracking}. The remaining pole was placed at 30\% larger from the origin (because this is only a single pole, it must necessarily be on the real axis).
Hence, the resulting poles were placed in the at $-16.5 \pm 8.3j, -26 + 0j$. A value of $h = \SI{0.0192}{\second}$ was chosen for the sampling period, based on the time constant of the fastest pole.

Because state feedback leaves the zeros and gain of the system unaffected, the magnitude of its DC-gain cannot be influenced arbitrarily. Therefore, to avoid steady-state errors, the feedforward gain $L_c$ was chosen as the inverse of the steady-state gain of the closed-loop system. This will be treated in greater detail in \cref{sec:q12}.
\begin{figure}[ht]
    \centering
    \includegraphics{media/q5/dt_step.eps}
    \caption{}
    \label{fig:q5_step}
\end{figure}
Using the desired pole locations mapped to the $z$-plane, the feedback gain $L$ can be determined using the \textsc{Matlab}-command \texttt{place}. \Cref{fig:q5_step} also shows the response from some other pole locations; faster and slower, and with more or less damping. The response resulting from the discussed design process is also shown. \Cref{fig:cont_plant_pzmap} shows all the corresponding pole and zero locations in the $z$-plane; clearly the zeros remain in place regardless of the pole-placement controller design.
\begin{figure}[ht]
    \centering
    \includegraphics{media/q5/dt_pzmap.eps}
    \caption{}
    \label{fig:q5_pzmap}
\end{figure}

\subsection{Output feedback using pole-placement\textnormal{\phantom{xxx}(Question 6)}}
\label{sec:ss_output}
\subsubsection{Servo problem}
Now, it will no longer be assumed that all the states are known, but that the controller can only see the output of the system. To apply state feedback, the controller must include an observer that provides an estimate of the current state. \Cref{fig:q6_block_servo} shows how the plant and the controller are connected. To do so, the \textsc{Matlab} command \texttt{lft} (linear fractional transformation) was used.
\begin{figure}[ht]
    \centering
    \includegraphics[scale=1.3]{media/q6/block_outputfeedback-02.eps}
    \caption{}
    \label{fig:q6_block_servo}
\end{figure}
From \cref{fig:q6_block_servo}, it is straightforward to find the corresponding state-space models.
\begin{equation}
    \text{\textsc{Plant}}\qquad
    \begin{aligned}
        x(k+1) &= \Phi x(k) + \mqty(0&\Gamma)\mqty(r(k)\\u(k))\\
        \mqty(y(k)\\u(k)\\y(k)\\r(k)) &= \mqty(C\\0\\C\\0)x(k) + \mqty(0&0\\0&1\\0&0\\1&0) \mqty(r(k)\\u(k))
    \end{aligned}
\end{equation}
\phantom{x}
\begin{equation}
    \text{\textsc{Controller}}\qquad
    \begin{aligned}
        \hat{x}(k+1) =& (\Phi - KC)\hat{x}(k) + \mqty(\Gamma&K&0)\mqty(u(k)\\y(k)\\r(k))\\
        u(k) =& -L\hat{x}(k) + \mqty(0&0&L_c)\mqty(u(k)\\y(k)\\r(k))
    \end{aligned}
\end{equation}
The same pole locations were chosen as those designed in \cref{sec:ss_full} to obtain the state feedback gain $L$. It is common practice to place the observer poles faster than the controller, such that the state estimation error will decay faster to zero. The rationale behind this is perspicuous: it does not make sense to apply a state feedback law based on a state estimation that is way off. Therefore, the observer poles were chosen to be a factor 1.3 times the state feedback poles (in the $s$-plane). Translated to the $z$-plane, this resulted in:
$$0.6473 \pm 0.1370j \quad  0.5220$$
Again, the feedforward gain $L_c$ was used to eliminate the steady-state error; its computation will be discussed in \cref{sec:q12}. The resulting closed-loop system was simulated with a step reference input and the state estimates initialised at $\hat{x}_0 = \mqty(-20 & 30 & 50)$. The result is shown in \cref{fig:q6_output_servo}; both the output and the state estimation error are displayed. The state error decays to zero faster than the output attains it final value due to the faster observer poles. Also, there is only a marginal difference between the output feedback controller and the full information case from \cref{sec:ss_full}.
\begin{figure}[ht!]
    \centering
    \includegraphics[]{media/q6/output_servo.eps}
    \caption{}
    \label{fig:q6_output_servo}
\end{figure}

\subsubsection{Disturbance rejection}
Now, the output feedback controller will be used to reject a step load disturbance. Because the disturbance is not directly measurable and also not controllable by $u$, the controller must be equipped with an observer of the disturbance; only then will it have the ability to reject it succesfully. Since a step disturbance is assumed, the disturbance model simply reads:
                                $$v(k + 1) = v(k)$$
in combination with an initial condition $v_0$ (for the unit step disturbance, $v_0 = 1$). To turn this model into an observer, the mismatch between the plant output and the estimated output will be fed as an input to the observer, multiplied by a gain $K_w$. Finally, the output of the controller $u(k)$ is based on the feedback law:
                                $$u(k) = -L\hat{x}(k) - \hat{v}(k)$$
which means that the state feedback law is applied on the state estimate, after which the disturbance estimate is subtracted. To build this system, the \textsc{Matlab} command \texttt{lft} was used in a similar fashion as before. \Cref{fig:q6_block_distrej} shows the layout of the (generalised) plant and controller.
\begin{figure}[ht!]
    \centering
    \includegraphics[scale=1.3]{media/q6/block_distrej-03.eps}
    \caption{}
    \label{fig:q6_block_distrej}
\end{figure}
From \cref{fig:q6_block_distrej}, the state-space models of the plant and controller can be readily derived.
\begin{equation}
    \text{\textsc{Plant}}\qquad
    \begin{aligned}
        x(k+1) =& \Phi x(k) + \mqty(\Gamma&\Gamma)\mqty(v(k)\\u(k))\\
        \mqty(y(k)\\u(k)\\y(k)) =& \mqty(C\\0\\C)x(k) + \mqty(0&0\\0&1\\0&0)\mqty(v(k)\\u(k))\\ 
    \end{aligned}
\end{equation}
\phantom{x}
\begin{equation}
    \text{\textsc{Controller}}\qquad
    \begin{aligned}
        \mqty(\hat{x}(k)\\\hat{v}(k)) =& \mqty(\Phi-KC & \Gamma\\-K_wC&1)\mqty(\hat{x}(k)\\\hat{v}(k)) + \mqty(\Gamma&K\\0&K_\omega)\mqty(u(k)\\y(k))\\
        u(k) =& \mqty(-L&-1)\mqty(\hat{x}(k)\\\hat{v}(k))
    \end{aligned}
\end{equation}
The choice of $K_w$ entails the placement of an additional observer pole; the value of $K_w$ can be computed together with the state observer gain $K$ using the \textsc{Matlab} command \texttt{place} for the augmented controller system. This pole was placed at $s = -14$ or $z = 0.76$. The other observer and controller poles were kept at the same locations since they showed adequate performance. The sampling time is also the same as for the servo controller.
\Cref{fig:q6_output_distrej} displays two plots; the top one is the reaction of the output to the step disturbance, the bottom plot is the estimate of the disturbance (which should of course converge to 1). The state estimates were initialised at $\hat{x}_0 = \mqty(1 & 3 & 5)$, the initial disturbance estimate was $\hat{v}_0 = -1$.
\begin{figure}[ht!]
    \centering
    \includegraphics[scale=1]{media/q6/output_distrej.eps}
    \caption{}
    \label{fig:q6_output_distrej}
\end{figure}
Clearly, the controller does a good job to reject the disturbance. The estimate suffers from a little overshoot but then converges quickly to 1. From the evolution of output it can be seen that initially not much happens, this is because the negative disturbance estimate and the state feedback cancel each other out. When the disturbance estimate starts to converge, the output of the system starts changing as well before it eventually settles after ca. \SI{0.5}{\second}.
\subsection{Full-information feedback using LQR\textnormal{\phantom{xxx}(Question 7)}}
\label{sec:ss_lqr}
Manual placement of the poles can sometimes be nonintuitive or overly complicated, especially for higher order systems. A potential solution for this issue exists in the form of the Linear Quadratic Regulator solution, which finds the optimal state feedback gain that minimises the cost function:\footnote{Although one could potentially also include a cross term involving both $u(k)$, $x(k)$, it is not frequently used in practice and therefore ommitted.}
\begin{equation}
    J = \sum^\infty_{k=1} x(k)^\top Q x(k) + u(k)^\top R u(k)
    \label{eq:lqrcost}
\end{equation}
where $R\succ 0$ and $Q \succeq 0$ are weighting matrices that can be chosen as part of the controller design. From \cref{eq:lqrcost} it is quite clear that $Q$ puts a penalty on nonzero states while $R$ will penalise the controller effort. The main interest is to drive the output $y$ to zero as fast as possible, which is why it makes sense to base $Q$ on $C$:
                $$ Q_0 = CC^\top $$
Scalar multiples of $Q_0$ can then be chosen to put a greater (or perhaps smaller) penalty on the tracking error. Given that the resulting full state feedback controller must be able to track a step reference, a similar scheme is used as shown in \cref{fig:q6_block_servo} with a feedforward gain $L_c$ that acts on the reference. How $L_c$ is determined will be discussed in \cref{sec:q12}. 
\begin{figure}
    \centering
    \includegraphics[scale=1]{media/q7/lqr_comp.eps}
    \caption{}
    \label{fig:q7_lqr_comp}
\end{figure}
\Cref{fig:q7_lqr_comp} shows the resulting step response for several choices of $Q$ and $R$. As one might expect, the fastest response is for the controller with the lowest penalty on the control effort and the highest penalty on the offset of the output. For now, no restrictions have been imposed on the controller effort; therefore the feedback law that corresponds with $Q = \num{5e4}\times Q_0$ and $R = 1$ has been chosen as the best performing LQR controller. The sampling period used for the simulations is again $h = \SI{0.0192}{\second}$.